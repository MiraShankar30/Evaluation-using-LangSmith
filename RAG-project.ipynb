{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "289fef6e-d946-4a63-bbb1-477f67756378",
   "metadata": {},
   "source": [
    "<b><h1>RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e045fb9d-a8e7-436b-93cd-d6c2ab6015d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "from langsmith import Client\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv()) \n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=str(os.getenv(\"LANGCHAIN_API_KEY\"))\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baff8914-52ee-40c9-b735-5486d4485db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client=Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3deb67-2d92-4cd3-ad39-22fa415298a7",
   "metadata": {},
   "source": [
    "Loading PDF from sample directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a4ddba-daae-4c43-8363-4567d0a6f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "directory_path = \"sample\"\n",
    "\n",
    "loader = DirectoryLoader(directory_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b502727-ba55-4d00-8dc3-35c0401d2de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbc6f775-31f8-4f60-86e7-f6bcc786dd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents:  348\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# split data\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "\n",
    "print(\"Total number of documents: \",len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff76d3d7-140b-40cf-94cc-9a6f3da5ccae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Skia/PDF m133', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36 Edg/133.0.0.0', 'creationdate': '2025-02-19T08:37:02+00:00', 'title': 'Salt and Sodium - The Nutrition Source', 'moddate': '2025-02-19T08:37:02+00:00', 'source': 'sample\\\\Salt and Sodium - The Nutrition Source.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content='The Nutrition Source > Salt and Sodium\\nSalt and Sodium\\nSalt, also known as sodium chloride, is about 40% sodium and 60% chloride. It\\nflavors food and is used as a binder and stabilizer. It is also a food preservative,\\nas bacteria can’t thrive in the presence of a high amount of salt. The human body\\n\\uf409\\nTHE NUTRITION SOURCE\\n \\uf431')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b4d88-e119-490f-aeab-0f1c58f45d50",
   "metadata": {},
   "source": [
    "Creating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17b84ac6-c475-4b8a-94aa-ffb0f9212013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04656680300831795,\n",
       " -0.0376756377518177,\n",
       " -0.0274836253374815,\n",
       " -0.02519204653799534,\n",
       " 0.023942284286022186]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector = embeddings.embed_query(\"Hello World\")\n",
    "vector[:5]\n",
    "# sample vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d843e376-af85-4197-a0f4-36db1567ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=docs, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945149ee-0a01-4f28-9ada-ab9e94ecdda3",
   "metadata": {},
   "source": [
    "Retrieving Top 10 similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb95ac92-a670-4c70-8684-bd27e2470cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "retrieved_docs = retriever.invoke(\"What are the types of salts?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d53f7f6-97af-490c-af85-00c9e6d08699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "222f7e33-a676-4b4b-8b79-021a96fabea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires a small amount of sodium to conduct nerve impulses, contract and\n",
      "relax muscles, and maintain the proper balance of water and minerals. It is\n",
      "estimated that we need about 500 mg of sodium daily for these vital functions.\n",
      "But too much sodium in the diet can lead to high blood pressure, heart disease,\n",
      "and stroke. It can also cause calcium losses, some of which may be pulled from\n",
      "bone. Most Americans consume at least 1.5 teaspoons of salt per day, or about\n",
      "3400 mg of sodium, which contains far more than our bodies need.\n",
      "Recommended Amounts\n",
      "The U.S. Dietary Reference Intakes state that there is not enough evidence to\n",
      "establish a Recommended Dietary Allowance or a toxic level for sodium (aside\n",
      "from chronic disease risk). Because of this, a Tolerable Upper intake Level (UL)\n",
      "has not been established; a UL is the maximum daily intake unlikely to cause\n",
      "harmful effects on health. \n",
      "Guidelines for Adequate Intakes (AI) of sodium were established based on the\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[5].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13e5ab1-7f55-402c-be8c-3e324a250061",
   "metadata": {},
   "source": [
    "LLM - Gemini model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3655be79-415c-4796-8c71-87f92090d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",temperature=0.3, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4093c70a-f89d-4c34-a440-61720aa697a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe6aa9ba-d87b-4219-a1da-d16521939789",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502fb822-c324-4b8b-8f1c-a8bf3ab98580",
   "metadata": {},
   "source": [
    "Sample RAG model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "768c0e4a-4f19-4fd4-9ab3-7fcf518bcac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iodized table salt, Kosher salt, Diamond Crystal kosher salt, sea salt, pink (Himalayan) salt, black salt, fleur de sel, and potassium salt (salt substitute).\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"list only the types of salt\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e5873f0-b066-4375-97fe-89e3157f19b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document does not contain the answer to what RAG is.  It discusses dietary recommendations for various nutrients, including zinc, and its relationship with age-related macular degeneration.  It also mentions different intake levels like RDA, AI, EAR, and UL.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is RAG?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59185866-cff7-497d-ba55-1b1082d8e005",
   "metadata": {},
   "source": [
    "<H1><B>Dataset: Manually Curated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f999e86-c38b-4378-8f2a-6d1b7e8c9d69",
   "metadata": {},
   "source": [
    "Created a dataset of question-answer pairs on a blog post about Vitamins and minerals.\n",
    "\n",
    "Built a Manually Curated dataset of input, output pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a262036a-5a7e-454e-b30d-1d637399e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# Define your QA pairs\n",
    "inputs = [\n",
    "    \"What is Water-soluble vitamins?\",\n",
    "    \"What is the source of Vitamin E?\",\n",
    "    \"What is the RDA of Vitamin B2?\",\n",
    "    \"List the fat soluble vitamins\",\n",
    "    \"What are the sources of Zinc?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"These include the eight B-complex vitamins (B1, B2, B3, B5, B6, B7, B9, B12) and vitamin C. Water-soluble vitamins are not stored in large amounts and need to be replenished regularly through your diet, as excess amounts are excreted through urine.\",\n",
    "    \"Nuts, seeds, spinach, and vegetable oils\",\n",
    "    \"1.3 mg/day (men), 1.1 mg/day (women)\",\n",
    "    \"Vitamin - A, D, B2, and C\",\n",
    "    \"Meat, shellfish, legumes, seeds\",\n",
    "    \n",
    "]\n",
    "\n",
    "# Create QA pairs\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "\n",
    "# Initialize Langsmith client\n",
    "client = Client()\n",
    "\n",
    "# Define dataset parameters\n",
    "dataset_name = \"Vit_Min_Dataset\"\n",
    "dataset_description = \"QA pairs about Vitamins and Minerals.\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=dataset_description,\n",
    ")\n",
    "\n",
    "# Add examples to the dataset\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f757297d-1d84-49fc-b372-15e6f8aac57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"1.png\" width=\"1000\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module \n",
    "from IPython.display import Image \n",
    "  \n",
    "# get the image \n",
    "Image(url=\"1.png\", width=1000, height=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb11efdc-f5a6-4e49-91cd-fe49ba57c546",
   "metadata": {},
   "source": [
    "<B><H1>Dataset: From User Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab839b23-5996-4698-a6f7-2eaa315b261e",
   "metadata": {},
   "source": [
    "Save user logs as a dataset for future testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ecd65f-b84f-4f47-aaab-b6bde1b08461",
   "metadata": {},
   "source": [
    "Loaded data from a site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ef3c066-809a-49c2-8581-f27681e2f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://drhyman.com/blogs/content/supplements-101-essential-vitamins-and-minerals#:~:text=Learn%20why%20the%20RDA%20doesn%E2%80%99t%20provide%20optimal%20intake,Dr.%20Mark%20Hyman%E2%80%99s%20patient-proven%20recommendations%20for%20peak%20health.\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "text = [p.text for p in soup.find_all(\"p\")]\n",
    "full_text = \"\\n\".join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ab30a2-09d1-477d-a092-e3e8b2c8984a",
   "metadata": {},
   "source": [
    "Q&A function - using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1787bad-ae35-43cd-ac77-d0b525f2a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Generates answers to user questions based on a provided website text using Gemini API.\n",
    "\n",
    "    Parameters:\n",
    "    inputs (dict): A dictionary with a single key 'question', representing the user's question as a string.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with a single key 'output', containing the generated answer as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # System prompt\n",
    "    system_msg = (\n",
    "        f\"Answer user questions in 2-3 sentences about this context: \\n\\n\\n {full_text}\"\n",
    "    )\n",
    "\n",
    "    # Pass in website text\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": inputs[\"question\"]},\n",
    "    ]\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\") \n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Response in output dict\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ccb63f-f4a7-4cb5-9742-c448128d0928",
   "metadata": {},
   "source": [
    "Sample Outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "179fe7ab-19e4-4fe0-aa98-f3a7b77bc8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"Magnesium can be found in foods like pumpkin seeds, almonds, spinach, and dark chocolate.  It's also available as a supplement in different forms, including citrate, glycinate, and L-threonate, each with specific benefits.\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(\n",
    "    {\n",
    "        \"question\": \"What is the source of Magnesium\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9188118-61a2-4ee0-90aa-02d3057c6226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"The Mark's Recommended Intake (MRI) for Vitamin A is 2000-3000 mcg/day. This contrasts with the Recommended Dietary Allowance (RDA) of 900 mcg/day for men and 700 mcg/day for women, which are lower amounts aimed at preventing deficiency rather than optimizing health.\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(\n",
    "    {\n",
    "        \"question\": \"What is the MRI for Vitamin-A\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b5f31c-0a01-45d6-8cfe-0d51db234480",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(\n",
    "    {\n",
    "        \"question\": \"What is the recommended dietary allowance of Iron for men\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18949e6-17f2-4f47-a502-fc396ed8b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(\n",
    "    {\n",
    "        \"question\": \"What is Fat-soluble vitamins?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725dc35a-9e68-44cc-893c-70d624c6b294",
   "metadata": {},
   "source": [
    "<B><H1>LLM-as-Judge: Built-in evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1261f23b-08a9-47cc-b569-e47dea7dda72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"2.png\" width=\"1000\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"2.png\", width=1000, height=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d73b719-3a36-4085-80a1-fd65725dcd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# Evaluators\n",
    "qa_evalulator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "dataset_name = \"Vit_Min_Dataset\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    answer_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-VitMindataset-qa\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into OpenAI\",\n",
    "    },\n",
    ") \n",
    "#Since it uses OpenAI as the LLM, not able to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7a1af-6b79-4a70-aa3c-63d5097b345a",
   "metadata": {},
   "source": [
    "<b><H1>Custom evaluator - Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "930b060d-6945-48a0-bb96-fa69213e5829",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"3.png\" width=\"1000\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"3.png\", width=1000, height=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfb9e0cb-1e2d-49d4-b744-dec30e3337fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-vitmin-qa-custom-eval-is-answered-e2ccd1f3' at:\n",
      "https://smith.langchain.com/o/a87e4dfa-61d0-4714-8c72-64f2925b822e/datasets/b5ef57fa-3d8f-49f7-b239-0e90a25f508c/compare?selectedSessions=23a1b317-2d22-4770-aace-b5cbe447574e\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f94ea3d43848c0b2acd335e7a51668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "from langsmith import evaluate\n",
    "\n",
    "def is_answered(run: Run, example: Example) -> dict:\n",
    "    # Get outputs\n",
    "    student_answer = run.outputs.get(\"answer\")\n",
    "\n",
    "    # Check if the student_answer is an empty string\n",
    "    if not student_answer:\n",
    "        return {\"key\": \"is_answered\", \"score\": 0}\n",
    "    else:\n",
    "        return {\"key\": \"is_answered\", \"score\": 1}\n",
    "\n",
    "\n",
    "# Evaluators\n",
    "qa_evalulator = [is_answered]\n",
    "dataset_name = \"Vit_Min_Dataset\"\n",
    "\n",
    "# Run\n",
    "experiment_results = evaluate(\n",
    "    answer_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-vitmin-qa-custom-eval-is-answered\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into gemini\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c854032-e37e-4137-9501-ebc6a5963fc0",
   "metadata": {},
   "source": [
    "Used a simple rule based function, which gives a score 1 - if the LLM gives response and score 0 - if the LLM doesn't give any response.<br>\n",
    "Output displayed in Langsmith:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ddd418d-e70b-4f56-944b-adbbefad08c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"4.png\" width=\"1000\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"4.png\", width=1000, height=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b323d-33cb-4d5a-b89f-0aaa4213757b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<B><H1>Custom evaluator - LLM as a judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93eb7d03-ace4-48c5-93b4-8eaad34564b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"5.png\" width=\"1000\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"5.png\", width=1000, height=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ef85bcd-db24-4270-bc29-0e88eaa5f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_semantic_similarity(run: Run, example: Example) -> dict:\n",
    "    input_question = example.inputs.get(\"question\")\n",
    "    reference_response = example.outputs.get(\"answer\")\n",
    "    run_response = run.outputs.get(\"answer\")\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0) # Using a low temperature for determinism.\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a semantic similarity evaluator. Compare the meanings of two responses to a question, \n",
    "    Reference Response and New Response, where the reference is the correct answer, and we are trying to judge if the new response is similar. \n",
    "    Provide a score between 1 and 10, where 1 means completely unrelated, and 10 means identical in meaning.\n",
    "\n",
    "    Question: {input_question}\n",
    "    Reference Response: {reference_response}\n",
    "    Run Response: {run_response}\n",
    "\n",
    "    Please provide your response as an integer value between 1 and 10, where 1 means unrelated and 10 means identical.\n",
    "    \"\"\"\n",
    "    similarity_score = llm.invoke(prompt)\n",
    "    return {\"key\": \"semantic_similarity\", \"score\": int(similarity_score.content)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50117587-784e-4d9f-b245-eea3cff4d5f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-vitmin-qa-custom-eval-simantic-similarity-673a2fd2' at:\n",
      "https://smith.langchain.com/o/a87e4dfa-61d0-4714-8c72-64f2925b822e/datasets/b5ef57fa-3d8f-49f7-b239-0e90a25f508c/compare?selectedSessions=42fab29c-2698-4531-8b45-5a290d9d5ae7\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694aeedfdb4c4fb3bed44df26a72f866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Error running evaluator <DynamicRunEvaluator compare_semantic_similarity> on run 7ffe627e-e3fb-4879-98a6-26db1051c747: ResourceExhausted('Resource has been exhausted (e.g. check quota).')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 629, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 626, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\Temp\\ipykernel_20952\\60810822.py\", line 19, in compare_semantic_similarity\n",
      "    similarity_score = llm.invoke(prompt)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 284, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 860, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 690, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 925, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 951, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 196, in _chat_with_retry\n",
      "    return _chat_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 336, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 194, in _chat_with_retry\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 178, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 835, in generate_content\n",
      "    response = rpc(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 293, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 153, in retry_target\n",
      "    _retry_error_helper(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 212, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 144, in retry_target\n",
      "    result = target()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Error running target function: 429 Resource has been exhausted (e.g. check quota).\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1914, in _forward\n",
      "    fn(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 629, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 626, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\Temp\\ipykernel_20952\\3523668270.py\", line 25, in answer_question\n",
      "    response = llm.invoke(messages)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 284, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 860, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 690, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 925, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 951, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 196, in _chat_with_retry\n",
      "    return _chat_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 336, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 194, in _chat_with_retry\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 178, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 835, in generate_content\n",
      "    response = rpc(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 293, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 153, in retry_target\n",
      "    _retry_error_helper(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 212, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 144, in retry_target\n",
      "    result = target()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Error running evaluator <DynamicRunEvaluator compare_semantic_similarity> on run 9ad268da-3a46-4130-a3e1-9ad747b19738: ResourceExhausted('Resource has been exhausted (e.g. check quota).')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 629, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 626, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\Temp\\ipykernel_20952\\60810822.py\", line 19, in compare_semantic_similarity\n",
      "    similarity_score = llm.invoke(prompt)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 284, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 860, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 690, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 925, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 951, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 196, in _chat_with_retry\n",
      "    return _chat_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 336, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 194, in _chat_with_retry\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 178, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 835, in generate_content\n",
      "    response = rpc(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 293, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 153, in retry_target\n",
      "    _retry_error_helper(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 212, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 144, in retry_target\n",
      "    result = target()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Error running target function: 429 Resource has been exhausted (e.g. check quota).\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1914, in _forward\n",
      "    fn(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 629, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 626, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\Temp\\ipykernel_20952\\3523668270.py\", line 25, in answer_question\n",
      "    response = llm.invoke(messages)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 284, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 860, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 690, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 925, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 951, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 196, in _chat_with_retry\n",
      "    return _chat_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 336, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 194, in _chat_with_retry\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 178, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 835, in generate_content\n",
      "    response = rpc(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 293, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 153, in retry_target\n",
      "    _retry_error_helper(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 212, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 144, in retry_target\n",
      "    result = target()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Error running evaluator <DynamicRunEvaluator compare_semantic_similarity> on run d60737bd-4d81-48b2-a047-31bfad2117f7: ResourceExhausted('Resource has been exhausted (e.g. check quota).')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 629, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 626, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\Temp\\ipykernel_20952\\60810822.py\", line 19, in compare_semantic_similarity\n",
      "    similarity_score = llm.invoke(prompt)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 284, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 860, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 690, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 925, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 951, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 196, in _chat_with_retry\n",
      "    return _chat_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 336, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 194, in _chat_with_retry\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 178, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 835, in generate_content\n",
      "    response = rpc(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 293, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 153, in retry_target\n",
      "    _retry_error_helper(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 212, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 144, in retry_target\n",
      "    result = target()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Error running target function: 429 Resource has been exhausted (e.g. check quota).\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1914, in _forward\n",
      "    fn(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 629, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 626, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\Temp\\ipykernel_20952\\3523668270.py\", line 25, in answer_question\n",
      "    response = llm.invoke(messages)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 284, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 860, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 690, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 925, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 951, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 196, in _chat_with_retry\n",
      "    return _chat_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 336, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 194, in _chat_with_retry\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 178, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 835, in generate_content\n",
      "    response = rpc(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 293, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 153, in retry_target\n",
      "    _retry_error_helper(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 212, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 144, in retry_target\n",
      "    result = target()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Error running evaluator <DynamicRunEvaluator compare_semantic_similarity> on run 317d33d9-860d-4bcc-b8fe-92118d7e9bdf: ResourceExhausted('Resource has been exhausted (e.g. check quota).')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 629, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 626, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\Temp\\ipykernel_20952\\60810822.py\", line 19, in compare_semantic_similarity\n",
      "    similarity_score = llm.invoke(prompt)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 284, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 860, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 690, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 925, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 951, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 196, in _chat_with_retry\n",
      "    return _chat_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 336, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 475, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 376, in iter\n",
      "    result = action(retry_state)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tenacity\\__init__.py\", line 478, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 194, in _chat_with_retry\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 178, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 835, in generate_content\n",
      "    response = rpc(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 293, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 153, in retry_target\n",
      "    _retry_error_helper(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 212, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 144, in retry_target\n",
      "    result = target()\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    }
   ],
   "source": [
    "# Evaluators\n",
    "qa_evalulator1 = [compare_semantic_similarity]\n",
    "dataset_name = \"Vit_Min_Dataset\"\n",
    "\n",
    "# Run\n",
    "experiment_results = evaluate(\n",
    "    answer_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator1,\n",
    "    experiment_prefix=\"test-vitmin-qa-custom-eval-simantic-similarity\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into gemini\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95620a08-a71a-41a8-8445-2d25e1e1854d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"6.png\" width=\"1000\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"6.png\", width=1000, height=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36edfa04-98c9-4f52-b5c9-2034592ed725",
   "metadata": {},
   "source": [
    "Semantic similarity is compared for the ground truth and the Output by the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c175326a-3244-4e4b-9d44-7f82f8c4a426",
   "metadata": {},
   "source": [
    "<b><h1> Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a351a0cf-31bd-44f7-98b9-fd391ca32ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aba1e8c3-720e-410c-943f-45bc62bf4104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "import re\n",
    "def compare_semantic_similarity_mistral(run: Run, example: Example) -> dict:\n",
    "    input_question = example.inputs.get(\"question\")\n",
    "    reference_response = example.outputs.get(\"answer\")\n",
    "    run_response = run.outputs.get(\"answer\")\n",
    "\n",
    "    model=\"mistral-large-latest\"\n",
    "    mistral_api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "    client = Mistral(api_key=mistral_api_key)\n",
    "    \n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a semantic similarity evaluator. Compare the meanings of two responses to a question, \n",
    "    Reference Response and New Response, where the reference is the correct answer, and we are trying to judge if the new response is similar. \n",
    "    Provide a score between 1 and 10, where 1 means completely unrelated, and 10 means identical in meaning.\n",
    "\n",
    "    Question: {input_question}\n",
    "    Reference Response: {reference_response}\n",
    "    Run Response: {run_response}\n",
    "\n",
    "    Please provide your response as an integer value between 1 and 10, where 1 means unrelated and 10 means identical.\n",
    "    \"\"\"\n",
    "    similarity_score_m = client.chat.complete(model=model,\n",
    "                                     messages = [\n",
    "                                        {\n",
    "                                            \"role\":\"user\",\n",
    "                                            \"content\":prompt,\n",
    "                                        },\n",
    "                                     ]\n",
    "                                    )\n",
    "    response_text = similarity_score_m.choices[0].message.content\n",
    "    match = re.search(r'\\b(10|[1-9])\\b', response_text)  # Find a number between 1 and 10\n",
    "\n",
    "    if match:\n",
    "        score = int(match.group(1))  # Convert the matched string to an integer\n",
    "        return {\"key\": \"semantic_similarity\", \"score\": score}\n",
    "    else:\n",
    "        return {\"key\": \"semantic_similarity\", \"score\": 0}  # Return 0 if no score is found\n",
    "    return {\"key\": \"semantic_similarity\", \"score\": similarity_score_m.choices[0].message.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "936c8542-dfd0-4021-a715-82aa70b38df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-vitmin-qa-custom-eval-simantic-similarity-mistral-e4b28c0a' at:\n",
      "https://smith.langchain.com/o/a87e4dfa-61d0-4714-8c72-64f2925b822e/datasets/b5ef57fa-3d8f-49f7-b239-0e90a25f508c/compare?selectedSessions=76bfa80e-3948-4124-b874-99ba9d860eda\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7688e8e790d6429e93d6a89479ebf3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluators\n",
    "qa_evalulator1 = [compare_semantic_similarity_mistral]\n",
    "dataset_name = \"Vit_Min_Dataset\"\n",
    "\n",
    "# Run\n",
    "experiment_results = evaluate(\n",
    "    answer_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator1,\n",
    "    experiment_prefix=\"test-vitmin-qa-custom-eval-simantic-similarity-mistral\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into mistral\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cdc9f71-0b0a-44c2-8d8c-2e21dfe025ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"7.png\" width=\"1000\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"7.png\", width=1000, height=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbcc003-b556-4088-b770-24fbcd52b8c8",
   "metadata": {},
   "source": [
    "<b><h1> Experiment on datasets from the prompt playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd7fc34e-0a0c-4c65-b355-bc7f5cc48e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    {\n",
    "        \"question\": \"Sea moss\",\n",
    "        \"doc_txt\": \"What potential health risk is associated with excessive iodine intake from sea moss?\",\n",
    "    },\n",
    "    {\"question\": \"hallucinations\", \"doc_txt\": \"What are the primary natural food sources of thiamin?\"},\n",
    "    {\n",
    "        \"question\": \"zinc deficiency\",\n",
    "        \"doc_txt\": \"What type of surgery or medical conditions can lead to zinc deficiency\",\n",
    "    },\n",
    "]\n",
    "\n",
    "outputs = [\"yes\", \"no\", \"yes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b50c6-4be5-40f1-aa32-39c1338d81ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Relevance_grade\"\n",
    "\n",
    "# Store\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Testing relevance grading.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df6d7d6-0e54-4b92-88b1-8ad066e9ef60",
   "metadata": {},
   "source": [
    "Used Gemini, Mistral and their different models to test the performance and compared them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86d6e5a4-9034-41d8-8c4f-96b73b640bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"8.png\" width=\"1000\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"8.png\", width=1000, height=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab835f-fd66-4a81-bac0-1eecb2417a61",
   "metadata": {},
   "source": [
    "<b><h1>Summary Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6eba2d8-3176-4a0a-99fb-35c8a1b7f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "def predict_gemini(inputs: dict) -> dict:\n",
    "    question=inputs[\"question\"]\n",
    "    document=inputs[\"doc_txt\"]\n",
    "    # Gemini Model\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0.3, max_tokens=500)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = f\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \n",
    "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "\n",
    "        Question: {question}\n",
    "        Reference Response: {document}\n",
    "        \"\"\"\n",
    "    score = llm.invoke(prompt)\n",
    "    return {\"grade\": score.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "743e8ad9-07de-41de-9da5-641c762525a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "def predict_mistral(inputs: dict) -> dict:\n",
    "    \n",
    "    model=\"mistral-large-latest\"\n",
    "    mistral_api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "    client = Mistral(api_key=mistral_api_key)\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n\n",
    "    If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score no premable or explaination.\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    "    )\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        question=inputs[\"question\"], document=inputs[\"doc_txt\"]\n",
    "    )\n",
    "\n",
    "    llm = client.chat.complete(model=model,\n",
    "                                     messages = [\n",
    "                                        {\n",
    "                                            \"role\":\"user\",\n",
    "                                            \"content\":prompt,\n",
    "                                        },\n",
    "                                     ]\n",
    "                                    )\n",
    "\n",
    "    grade = llm.choices[0].message.content\n",
    "    return {\"grade\": grade}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cfe3ab3-d3ed-419d-9435-85a13c2acf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langsmith.schemas import Example, Run\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "\n",
    "def f1_score_summary_evaluator(runs: List[Run], examples: List[Example]) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates the F1 score for a list of runs against a set of examples.\n",
    "\n",
    "    The function iterates through paired runs and examples, comparing the output\n",
    "    of each run (`run.outputs[\"grade\"]`) with the expected output in the example\n",
    "    (`example.outputs[\"answer\"]`). It calculates the true positives, false positives,\n",
    "    and false negatives based on these comparisons to compute the F1 score of the predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - runs (List[Run]): A list of run objects, where each run contains an output that is a prediction.\n",
    "    - examples (List[Example]): A list of example objects, where each example contains an output that is the expected answer.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with a single key-value pair where the key is \"f1_score\" and the value\n",
    "    \"\"\"\n",
    "\n",
    "    # Default values\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    # Iterate through samples\n",
    "    for run, example in zip(runs, examples):\n",
    "        reference = example.outputs[\"answer\"]\n",
    "        prediction = run.outputs[\"grade\"]\n",
    "        if reference and prediction == reference:\n",
    "            true_positives += 1\n",
    "        elif prediction and not reference:\n",
    "            false_positives += 1\n",
    "        elif not prediction and reference:\n",
    "            false_negatives += 1\n",
    "    if true_positives == 0:\n",
    "        return {\"key\": \"f1_score\", \"score\": 0.0}\n",
    "\n",
    "    # Compute F1 score\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"key\": \"f1_score\", \"score\": f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c3a82dc-1eec-4f4b-8c4f-0d855ecfcf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-score-mistral-5f6873bf' at:\n",
      "https://smith.langchain.com/o/a87e4dfa-61d0-4714-8c72-64f2925b822e/datasets/97e32f7e-ad98-4f6b-9d21-fa5c425aa83e/compare?selectedSessions=88145053-32bf-4490-8e2b-345a39da4c7b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d68e6108af4ead9da8577cdf6eae14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.doc_txt</th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.grade</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What type of surgery or medical conditions can...</td>\n",
       "      <td>zinc deficiency</td>\n",
       "      <td>yes</td>\n",
       "      <td>None</td>\n",
       "      <td>yes</td>\n",
       "      <td>7.327980</td>\n",
       "      <td>e05d8031-72ba-4aa7-ae50-fd04d1d540c2</td>\n",
       "      <td>531cec15-8e8e-40a8-a038-c81359aead7e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the primary natural food sources of t...</td>\n",
       "      <td>hallucinations</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "      <td>no</td>\n",
       "      <td>4.662165</td>\n",
       "      <td>7ef36323-2cb2-44b1-830b-c60de837cfb2</td>\n",
       "      <td>a310f261-0d58-48e3-96b5-691e627b3591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What potential health risk is associated with ...</td>\n",
       "      <td>Sea moss</td>\n",
       "      <td>yes</td>\n",
       "      <td>None</td>\n",
       "      <td>yes</td>\n",
       "      <td>4.565858</td>\n",
       "      <td>fb510641-52ba-4dd2-9461-f727848ddf9a</td>\n",
       "      <td>945d8e26-1359-4987-8375-521a30f2140b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults test-score-mistral-5f6873bf>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    predict_mistral,\n",
    "    data=\"Relevance_grade\",\n",
    "    summary_evaluators=[f1_score_summary_evaluator],\n",
    "    experiment_prefix=\"test-score-mistral\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"model\": \"mistral\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f9e26a7-55bd-424c-b365-d357c5e2565c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-score-gemini-7204bd0f' at:\n",
      "https://smith.langchain.com/o/a87e4dfa-61d0-4714-8c72-64f2925b822e/datasets/97e32f7e-ad98-4f6b-9d21-fa5c425aa83e/compare?selectedSessions=6e3c587c-5d56-4975-a218-5a8c36941a79\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8bed1c710f4a8da7a470623e3ad7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.doc_txt</th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.grade</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What type of surgery or medical conditions can...</td>\n",
       "      <td>zinc deficiency</td>\n",
       "      <td>yes</td>\n",
       "      <td>None</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.747617</td>\n",
       "      <td>e05d8031-72ba-4aa7-ae50-fd04d1d540c2</td>\n",
       "      <td>efdaa028-5334-4281-80a6-e93138ab72ba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the primary natural food sources of t...</td>\n",
       "      <td>hallucinations</td>\n",
       "      <td>no</td>\n",
       "      <td>None</td>\n",
       "      <td>no</td>\n",
       "      <td>1.806177</td>\n",
       "      <td>7ef36323-2cb2-44b1-830b-c60de837cfb2</td>\n",
       "      <td>21a4b319-a3ce-459a-912f-ad8d5fbb4090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What potential health risk is associated with ...</td>\n",
       "      <td>Sea moss</td>\n",
       "      <td>yes</td>\n",
       "      <td>None</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.862178</td>\n",
       "      <td>fb510641-52ba-4dd2-9461-f727848ddf9a</td>\n",
       "      <td>f4a86df0-7adf-4825-8e9f-c94c7c38c725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults test-score-gemini-7204bd0f>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    predict_gemini,\n",
    "    data=\"Relevance_grade\",\n",
    "    summary_evaluators=[f1_score_summary_evaluator],\n",
    "    experiment_prefix=\"test-score-gemini\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"model\": \"gemini\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6a8aaee-a09d-4fec-916e-1f5a7fcbc7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"9.png\" width=\"1000\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"9.png\", width=1000, height=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2113399d-6a68-485d-8a63-eed06f478227",
   "metadata": {},
   "source": [
    "<b><h1>RAG Application evaluation - Answer Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dcdc586c-6a49-4c4f-8cdb-8476d16b3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INDEX\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load\n",
    "url = \"https://python.langchain.com/v0.1/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bba5b2e9-b329-4664-982f-23dd1356654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from mistralai.client import MistralClient\n",
    "class RagBot:\n",
    "    def __init__(self, retriever, model: str = \"mistral-medium\"):\n",
    "        self._retriever = retriever\n",
    "        self._model = model\n",
    "\n",
    "    @traceable()\n",
    "    def retrieve_docs(self, question):\n",
    "        return self._retriever.invoke(question)\n",
    "\n",
    "    @traceable()\n",
    "    def get_answer(self, question: str):\n",
    "        similar = self.retrieve_docs(question)\n",
    "        similar_str = \"\\n\\n\".join([str(doc) for doc in similar]) #convert similar documents to a single string.\n",
    "        mistral_api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "        client = Mistral(api_key=mistral_api_key)\n",
    "        \n",
    "        prompt = f\"\"\"You are a helpful AI code assistant with expertise in LCEL. Use the following docs to produce a concise code solution to the user question.\\n \n",
    "        Docs: {similar_str} \"\"\"\n",
    "        \n",
    "        response = client.chat.complete(model=self._model,\n",
    "                                     messages = [\n",
    "                                        {\n",
    "                                            \"role\":\"user\",\n",
    "                                            \"content\":prompt,\n",
    "                                        },\n",
    "                                     ])\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"contexts\": [str(doc) for doc in similar],\n",
    "        }\n",
    "\n",
    "rag_bot = RagBot(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e5eb72a-18c2-4d0e-88b6-a6ae65fb7a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To create a simple LCEL chain that takes a topic and generates a joke using the OpenAI model, you can use the following code:\\n```python\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Create the model\\nmodel = ChatOpenAI(model=\"gpt-4\")\\n\\n# Create the prompt template\\ntemplate = \"tell me a short joke about {topic}\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Create the output parser\\noutput_parser = StrOutputParser()\\n\\n# Create the chain\\nchain = prompt | model | output_parser\\n\\n# Invoke the chain with a topic\\noutput = chain.invoke({\"topic\": \"ice cream\"})\\nprint(output)\\n```\\nThis code creates an LCEL chain with three components: a prompt template, a model, and an output parser. The `|` symbol is used to chain these components together. When the chain is invoked with a topic, the prompt template generates a prompt using the topic, the model generates a response to the prompt, and the output parser converts the model\\'s response into a string.\\n\\nNote that this code uses the OpenAI model, which requires an API key. You can obtain an API key by signing up for an account on the OpenAI website and following the instructions to create an API key. Once you have an API key, you can set the `OPENAI_API_KEY` environment variable to the value of your API key. Alternatively, you can pass the API key directly to the `ChatOpenAI` constructor using the `api_key` parameter.\\n\\nHere is an example of how to set the `OPENAI_API_KEY` environment variable:\\n```python\\nimport os\\nimport getpass\\n\\n# Prompt the user for their API key\\napi_key = getpass.getpass(prompt=\"Enter your OpenAI API key: \")\\n\\n# Set the environment variable\\nos.environ[\"OPENAI_API_KEY\"] = api_key\\n```\\nOnce you have set the `OPENAI_API_KEY` environment variable, you can use the `ChatOpenAI` constructor without passing an `api_key` parameter.\\n\\nHere is an example of how to pass the API key directly to the `ChatOpenAI` constructor:\\n```python\\nfrom langchain_openai import ChatOpenAI\\n\\n# Create the model with the API key\\nmodel = ChatOpenAI(model=\"gpt-4\", api_key=\"your_api_key_here\")\\n```\\nNote that the `ChatOpenAI` constructor also accepts a `model` parameter, which specifies the OpenAI model to use. In this example, we are using the `gpt-4` model, but you can use any of the available OpenAI models by specifying the appropriate model name.\\n\\nYou can also use other models besides OpenAI by installing the appropriate package and creating an instance of the model class. For example, here is how to create a chain that uses the Anthropic model:\\n```python\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Create the model\\nmodel = ChatAnthropic(model=\"claude-3-sonnet-20240229\")\\n\\n# Create the prompt template\\ntemplate = \"tell me a short joke about {topic}\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Create the output parser\\noutput_parser = StrOutputParser()\\n\\n# Create the chain\\nchain = prompt | model | output_parser\\n\\n# Invoke the chain with a topic\\noutput = chain.invoke({\"topic\": \"ice cream\"})\\nprint(output)\\n```\\nThis code is similar to the previous example, but it uses the `ChatAnthropic` class instead of the `ChatOpenAI` class to create the model. Note that you will need to install the `langchain-anthropic` package in order to use the Anthropic model. You can install it using the following command:\\n```\\npip install langchain-anthropic\\n```\\nHere is an example of how to create a chain that uses the Google Vertex AI model:\\n```python\\nfrom langchain_google_vertexai import ChatVertexAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Create the model\\nmodel = ChatVertexAI(model=\"gemini-pro\")\\n\\n# Create the prompt template\\ntemplate = \"tell me a short joke about {topic}\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Create the output parser\\noutput_parser = StrOutputParser()\\n\\n# Create the chain\\nchain = prompt | model | output_parser\\n\\n# Invoke the chain with a topic\\noutput = chain.invoke({\"topic\": \"ice cream\"})\\nprint(output)\\n```\\nThis code is similar to the previous examples, but it uses the `ChatVertexAI` class instead of the `ChatOpenAI` or `ChatAnthropic` classes to create the model. Note that you will need to set the `GOOGLE_API_KEY` environment variable to your Google Cloud Platform API key in order to use the Google Vertex AI model. You can obtain an API key by following the instructions in the [Google Cloud Platform documentation](https://cloud.google.com/docs/authentication/api-keys).\\n\\nHere is an example of how to create a chain that uses the Cohere model:\\n```python\\nfrom langchain_cohere import ChatCohere\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Create the model\\nmodel = ChatCohere(model=\"command-r\")\\n\\n# Create the prompt template\\ntemplate = \"tell me a short joke about {topic}\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Create the output parser\\noutput_parser = StrOutputParser()\\n\\n# Create the chain\\nchain = prompt | model | output_parser\\n\\n# Invoke the chain with a topic\\noutput = chain.invoke({\"topic\": \"ice cream\"})\\nprint(output)\\n```\\nThis code is similar to the previous examples, but it uses the `ChatCohere` class instead of the `ChatOpenAI`, `ChatAnthropic`, or `ChatVertexAI` classes to create the model. Note that you will need to set the `COHERE_API_KEY` environment variable to your Cohere API key in order to use the Cohere model. You can obtain an API key by signing up for a Cohere account and following the instructions in the [Cohere documentation](https://docs.cohere.ai/).\\n\\nHere is an example of how to create a chain that uses the Fireworks model:\\n```python\\nfrom langchain_fireworks import ChatFireworks\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Create the model\\nmodel = ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")\\n\\n# Create the prompt template\\ntemplate = \"tell me a short joke about {topic}\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Create the output parser\\noutput_parser = StrOutputParser()\\n\\n# Create the chain\\nchain = prompt | model | output_parser\\n\\n# Invoke the chain with a topic\\noutput = chain.invoke({\"topic\": \"ice cream\"})\\nprint(output)\\n```\\nThis code is similar to the previous examples, but it uses the `ChatFireworks` class instead of the `ChatOpenAI`, `ChatAnthropic`, `ChatVertexAI`, or `ChatCohere` classes to create the model. Note that you will need to set the `FIREWORKS_API_KEY` environment variable to your Fireworks API key in order to use the Fireworks model. You can obtain an API key by signing up for a Fireworks account and following the instructions in the [Fireworks documentation](https://docs.fireworkslab.com/).\\n\\nHere is an example of how to create a chain that uses the Mistral AI model:\\n```python\\nfrom langchain_mistralai import ChatMistralAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Create the model\\nmodel = ChatMistralAI(model=\"mistral-large-latest\")\\n\\n# Create the prompt template\\ntemplate = \"tell me a short joke about {topic}\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Create the output parser\\noutput_parser = StrOutputParser()\\n\\n# Create the chain\\nchain = prompt | model | output_parser\\n\\n# Invoke the chain with a topic\\noutput = chain.invoke({\"topic\": \"ice cream\"})\\nprint(output)\\n```\\nThis code is similar to the previous examples, but it uses the `ChatMistralAI` class instead of the `ChatOpenAI`, `ChatAnthropic`, `ChatVertexAI`, `ChatCohere`, or `ChatFireworks` classes to create the model. Note that you will need to set the `MISTRAL_API_KEY` environment variable to your Mistral AI API key in order to use the Mistral AI model. You can obtain an API key by signing up for a Mistral AI account and following the instructions in the [Mistral AI documentation](https://docs.mistral-ai.com/).\\n\\nHere is an example of how to create a chain that uses the Together AI model:\\n```python\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Create the model\\nmodel = ChatOpenAI(\\n    base_url=\"https://api.together.xyz/v1\",\\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\\n)\\n\\n# Create the prompt template\\ntemplate = \"tell me a short joke about {topic}\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Create the output parser\\noutput_parser = StrOutputParser()\\n\\n# Create the chain\\nchain = prompt | model | output_parser\\n\\n# Invoke the chain with a topic\\noutput = chain.invoke({\"topic\": \"ice cream\"})\\nprint(output)\\n```\\nThis code is similar to the previous examples, but it uses the `ChatOpenAI` class instead of the `ChatAnthropic`, `ChatVertexAI`, `ChatCohere`, `ChatFireworks`, or `ChatMistralAI` classes to create the model. Note that you will need to set the `TOGETHER\\\\_API\\\\_KEY` environment variable to your Together AI API key in order to use the Together AI model. You can obtain an API key by signing up for a Together AI account and following the instructions in the [Together AI documentation](https://docs.together.xyz/). Note that the `base_url` and `model` parameters are also set to the appropriate values for the Together AI model.\\n\\nOnce you have created a chain, you can invoke it by calling the `invoke` method and passing in a dictionary of input variables. The `invoke` method returns the output of the chain as a string. You can also use the `stream` method to stream the output of the chain as it is generated. The `stream` method returns an iterator that yields strings as the output is generated.\\n\\nHere is an example of how to use the `stream` method to stream the output of a chain:\\n```python\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Create the model\\nmodel = ChatOpenAI(model=\"gpt-4\")\\n\\n# Create the prompt template\\ntemplate = \"tell me a short joke about {topic}\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Create the output parser\\noutput_parser = StrOutputParser()\\n\\n# Create the chain\\nchain = prompt | model | output_parser\\n\\n# Invoke the chain with a topic and stream the output\\ninputs = {\"topic\": \"ice cream\"}\\nfor output in chain.stream(inputs):\\n    print(output, end=\"\")\\n```\\nThis code is similar to the previous example, but it uses the `stream` method instead of the `invoke` method to generate the output. The `stream` method returns an iterator that yields strings as the output is generated. The `print` function is used to print each string as it is yielded. Note that the `end` parameter of the `print` function is set to `\"\"` to prevent a newline from being printed after each string.\\n\\nOnce you have created a chain, you can also inspect it to see how it is composed. The `get_graph` method returns a graph representation of the chain, and the `get_prompts` method returns a list of the prompts used in the chain.\\n\\nHere is an example of how to use the `get_graph` method to inspect a chain:\\n```python\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Create the model\\nmodel = ChatOpenAI(model=\"gpt-4\")\\n\\n# Create the prompt template\\ntemplate = \"tell me a short joke about {topic}\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Create the output parser\\noutput_parser = StrOutputParser()\\n\\n# Create the chain\\nchain = prompt | model | output_parser\\n\\n# Get the graph representation of the chain\\ngraph = chain.get_graph()\\n\\n# Print the graph representation of the chain\\nprint(graph)\\n```\\nThis code is similar to the previous examples, but it uses the `get_graph` method to obtain a graph representation of the chain. The `print` function is used to print the graph representation.\\n\\nHere is an example of how to use the `get_prompts` method to inspect a chain:\\n```python\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Create the model\\nmodel = ChatOpenAI(model=\"gpt-4\")\\n\\n# Create the prompt template\\ntemplate = \"tell me a short joke about {topic}\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Create the output parser\\noutput_parser = StrOutputParser()\\n\\n# Create the chain\\nchain = prompt | model | output_parser\\n\\n# Get the list of prompts used in the chain\\nprompts = chain.get_prompts()\\n\\n# Print the list of prompts used in the chain\\nprint(prompts)\\n```\\nThis code is similar to the previous examples, but it uses the `get_prompts` method to obtain a list of the prompts used in the chain. The `print` function is used to print the list of prompts.\\n\\nIn addition to the basic LCEL components, there are also several advanced components that you can use to create more complex chains. For example, you can use the `RunnableParallel` component to run multiple components in parallel, or the `RunnablePassthrough` component to pass input variables directly to the output without modifying them. You can also use the `StrOutputParser` component to parse the output of a model into a string, or the `ChatPromptTemplate` component to generate a prompt using input variables.\\n\\nHere is an example of how to use the `RunnableParallel` component to run multiple components in parallel:\\n```python\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\\n\\n# Create the model\\nmodel = ChatOpenAI(model=\"gpt-4\")\\n\\n# Create the prompt template\\ntemplate = \"tell me a short joke about {topic}\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Create the output parser\\noutput_parser = StrOutputParser()\\n\\n# Create the parallel runnable\\nsetup_and_retrieval = RunnableParallel(\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n)\\n\\n# Create the chain\\nchain = setup_and_retrieval | prompt | model | output_parser\\n\\n# Invoke the chain with a topic and question\\ninputs = {\"topic\": \"ice cream\", \"question\": \"Why is it cold?\"}\\noutput = chain.invoke(inputs)\\nprint(output)\\n```\\nThis code is similar to the previous examples, but it uses the `RunnableParallel` component to run the `retriever` and `RunnablePassthrough` components in parallel. The `retriever` component is a hypothetical component that retrieves relevant documents for the input topic, and the `RunnablePassthrough` component passes the input question directly to the output without modifying it. The `RunnableParallel` component returns a dictionary containing the outputs of the `retriever` and `RunnablePassthrough` components, which are then passed as input to the `prompt` component.\\n\\nHere is an example of how to use the `StrOutputParser` component to parse the output of a model into a string:\\n```python\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Create the model\\nmodel = ChatOpenAI(model=\"gpt-4\")\\n\\n# Create the prompt template\\ntemplate = \"tell me a short joke about {topic}\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Create the output parser\\noutput_parser = StrOutputParser()\\n\\n# Create the chain\\nchain = prompt | model | output_parser\\n\\n# Invoke the chain with a topic\\ninputs = {\"topic\": \"ice cream\"}\\noutput = chain.invoke(inputs)\\nprint(output)\\n```\\nThis code is similar to the previous examples, but it uses the `StrOutputParser` component to parse the output of the `model` component into a string. The `StrOutputParser` component takes the output of the `model` component and converts it into a string, which is then returned as the output of the chain.\\n\\nHere is an example of how to use the `ChatPromptTemplate` component to generate a prompt using input variables:\\n```python\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Create the model\\nmodel = ChatOpenAI(model=\"gpt-4\")\\n\\n# Create the prompt template\\ntemplate = \"tell me a short joke about {topic}\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Create the output parser\\noutput_parser = StrOutputParser()\\n\\n# Create the chain\\nchain = prompt | model | output_parser\\n\\n# Invoke the chain with a topic\\ninputs = {\"topic\": \"ice cream\"}\\noutput = chain.invoke(inputs)\\nprint(output)\\n```\\nThis code is similar to the previous examples, but it uses the `ChatPromptTemplate` component to generate a prompt using input variables. The `ChatPromptTemplate` component takes a template string and replaces the `{variable}` placeholders with the corresponding input variables. In this example, the template string is `\"tell me a short joke about {topic}\"`, and the input variable is `\"topic\"`. The `ChatPromptTemplate` component generates a prompt using the input variable and passes it to the `model` component.\\n\\nIn addition to the basic and advanced LCEL components, you can also use the `configurable_alternatives` method to create a configurable chain that can be dynamically configured at runtime.\\n\\nHere is an example of how to use the `configurable_alternatives` method to create a configurable chain:\\n```python\\nfrom langchain_openai import ChatOpenAI, OpenAI\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import ConfigurableField\\n\\n# Create the OpenAI model\\nopenai_model = ChatOpenAI(model=\"gpt-4\")\\n\\n# Create the Anthropic model\\nanthropic_model = ChatAnthropic(model=\"claude-3-sonnet-20240229\")\\n\\n# Create the OpenAI LLM\\nopenai_llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\\n\\n# Create the prompt template\\ntemplate = \"tell me a short joke about {topic}\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Create the output parser\\noutput_parser = StrOutputParser()\\n\\n# Create the configurable field\\nconfigurable_field = ConfigurableField(id=\"model\")\\n\\n# Create the configurable alternatives\\nconfigurable_alternatives = openai_model.configurable_alternatives(\\n    configurable_field,\\n    openai=openai_llm,\\n    anthropic=anthropic_model,\\n    default_key=\"openai\",\\n)\\n\\n# Create the chain\\nchain = prompt | configurable_alternatives | output_parser\\n\\n# Invoke the chain with a topic and a model configuration\\ninputs = {\\n    \"topic\": \"ice cream\",\\n    \"config\": {\"model\": \"anthropic\"},\\n}\\noutput = chain.invoke(inputs)\\nprint(output)\\n```\\nThis code is similar to the previous examples, but it uses the `configurable_alternatives` method to create a configurable chain that can be dynamically configured at runtime. The `configurable_alternatives` method takes a `ConfigurableField` object and a dictionary of alternative models, and returns a runnable that can be configured to use any of the alternative models. In this example, the `configurable_field` object has an `id` of `\"model\"`, and the alternative models are `openai_llm` and `anthropic_model`. The `default_key` parameter is set to `\"openai\"`, which means that the `openai_llm` model will be used by default if no model configuration is provided.\\n\\nTo configure the chain to use a specific model, you can pass a `config` dictionary as input to the `invoke` method. The `config` dictionary should contain a key with the same `id` as the `ConfigurableField` object, and a value that corresponds to one of the alternative models. In this example, the `config` dictionary contains a key with an `id` of `\"model\"` and a value of `\"anthropic\"`, which means that the `anthropic_model` will be used.\\n\\nOverall, LCEL is a powerful and flexible framework for building complex chains of components that can be easily composed, inspected, and configured. With LCEL, you can quickly and easily build sophisticated pipelines that can process and analyze large amounts of data, and generate meaningful and actionable insights.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_bot.get_answer(\"What is LCEL?\")\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21bd9d15-95e5-4adb-97af-c1c84cf1d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0372d00-9669-4729-b947-0760251fb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "inputs = [\n",
    "    \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\",\n",
    "    \"How can I make the output of my LCEL chain a string?\",\n",
    "    \"How can I apply a custom function to one of the inputs of an LCEL chain?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"Use RunnablePassthrough. from langchain_core.runnables import RunnableParallel, RunnablePassthrough; from langchain_core.prompts import ChatPromptTemplate; from langchain_openai import ChatOpenAI; prompt = ChatPromptTemplate.from_template('Tell a joke about: {input}'); model = ChatOpenAI(); runnable = ({'input' : RunnablePassthrough()} | prompt | model); runnable.invoke('flowers')\",\n",
    "    \"Use StrOutputParser. from langchain_openai import ChatOpenAI; from langchain_core.prompts import ChatPromptTemplate; from langchain_core.output_parsers import StrOutputParser; prompt = ChatPromptTemplate.from_template('Tell me a short joke about {topic}'); model = ChatOpenAI(model='gpt-3.5-turbo') #gpt-4 or other LLMs can be used here; output_parser = StrOutputParser(); chain = prompt | model | output_parser\",\n",
    "    \"Use RunnableLambda with itemgetter to extract the relevant key. from operator import itemgetter; from langchain_core.prompts import ChatPromptTemplate; from langchain_core.runnables import RunnableLambda; from langchain_openai import ChatOpenAI; def length_function(text): return len(text); chain = ({'prompt_input': itemgetter('foo') | RunnableLambda(length_function),} | prompt | model); chain.invoke({'foo':'hello world'})\",\n",
    "]\n",
    "\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "\n",
    "# Create dataset\n",
    "client = Client()\n",
    "dataset_name = \"RAG_test_LCEL\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about LCEL.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ac487ef-d6b1-43e1-aa87-37adcccab103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "grade_prompt_hallucinations = prompt = hub.pull(\"langchain-ai/rag-answer-hallucination\")\n",
    "\n",
    "def answer_hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for generation hallucination\n",
    "    \"\"\"\n",
    "    \n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    contexts = run.outputs[\"contexts\"]\n",
    "        \n",
    "    # RAG answer \n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0.3, max_tokens=500)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_hallucinations | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"documents\": contexts,\n",
    "                                  \"student_answer\": prediction})\n",
    "    final_score = score[0][\"args\"][\"Score\"]\n",
    "    # print(score)\n",
    "\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": final_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26d4bc8d-8782-4bf9-9643-ec512e3328be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-gemini-hallucination-945c1fda' at:\n",
      "https://smith.langchain.com/o/a87e4dfa-61d0-4714-8c72-64f2925b822e/datasets/5a65df5b-e73a-420e-8939-2b0cacc52590/compare?selectedSessions=20efb0bc-223e-4296-b453-edde3260ea84\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35777391f084a5d83aad6dd888aa622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"RAG_test_LCEL\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_hallucination_evaluator],\n",
    "    experiment_prefix=\"rag-qa-gemini-hallucination\",\n",
    "    metadata={\n",
    "        \"variant\": \"LCEL context, gemini-1.5-flash\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5f5be03-8be3-4d93-b80b-aaae6aac22c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"10.png\" width=\"1000\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"10.png\", width=1000, height=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692dfb45-ba48-44d9-878b-0dc311118934",
   "metadata": {},
   "source": [
    "<b><h1>RAG Application evaluation - Document Relevance to Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8796134c-0cf0-4286-a06e-fdd547c1f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade prompt \n",
    "grade_prompt_doc_relevance = hub.pull(\"langchain-ai/rag-document-relevance\")\n",
    "\n",
    "def docs_relevance_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for document relevance\n",
    "    \"\"\"\n",
    "    \n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    contexts = run.outputs[\"contexts\"]\n",
    "        \n",
    "    # RAG answer \n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0.3, max_tokens=500)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_doc_relevance | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"question\":input_question,\n",
    "                                  \"documents\":contexts})\n",
    "    final_score = score[0][\"args\"][\"Score\"]\n",
    "\n",
    "    return {\"key\": \"document_relevance\", \"score\": final_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f571961-0a6f-4202-b5ec-8aac5ff710c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-gemini-doc-relevance-04b36138' at:\n",
      "https://smith.langchain.com/o/a87e4dfa-61d0-4714-8c72-64f2925b822e/datasets/5a65df5b-e73a-420e-8939-2b0cacc52590/compare?selectedSessions=6eb69359-333b-4f0a-bd22-fce12cb95094\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef66f829f3ae40db9d6f92101a99c549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"RAG_test_LCEL\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[docs_relevance_evaluator],\n",
    "    experiment_prefix=\"rag-qa-gemini-doc-relevance\",\n",
    "    metadata={\n",
    "        \"variant\": \"LCEL context, gemini-1.5-flash\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb699a82-e430-48e1-b1ba-77254889a213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"11.png\" width=\"1000\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"11.png\", width=1000, height=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84200511-800b-48c6-a0a2-e06c350cbb08",
   "metadata": {},
   "source": [
    "<b><h1>RAG Application Evaluator - Reference Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd4ad824-0c8c-4a68-984b-e92d5efd6af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3d971e8-30e3-41e1-b807-cd20344b164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "# Grade prompt \n",
    "grade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get summary\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    reference = example.outputs[\"answer\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0.3, max_tokens=500)\n",
    "\n",
    "    # Structured prompt\n",
    "    \n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"correct_answer\": reference,\n",
    "                                  \"student_answer\": prediction})\n",
    "    final_score = score[0][\"args\"][\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_score\", \"score\": final_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "536f45a7-2a6a-435e-90fb-1a4eee5f9de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-gemini-18ecd6f7' at:\n",
      "https://smith.langchain.com/o/a87e4dfa-61d0-4714-8c72-64f2925b822e/datasets/5a65df5b-e73a-420e-8939-2b0cacc52590/compare?selectedSessions=809420d5-7bfa-4acd-8079-f6362397b536\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b81c8ed8ce845e6a898e619645d551e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "dataset_name = \"RAG_test_LCEL\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=\"rag-qa-gemini\",\n",
    "    metadata={\"variant\": \"LCEL context, gemini-1.5.flash\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4001e5e-6c6e-4e60-9da8-6fd90dafbe25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"12.png\" width=\"1000\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"12.png\", width=1000, height=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9ed50-24da-4948-ab22-77019b651253",
   "metadata": {},
   "source": [
    "<B><H1>Regression testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d948b871-314d-4e64-b6e2-c5659d9d551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# QA\n",
    "inputs = [\n",
    "    \"My LCEL map contains the key 'question'. What is the difference between using itemgetter('question'), lambda x: x['question'], and x.get('question')?\",\n",
    "    \"How can I make the output of my LCEL chain a string?\",\n",
    "    \"How can I run two LCEL chains in parallel and write their output to a map?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"Itemgetter can be used as shorthand to extract specific keys from the map. In the context of a map operation, the lambda function is applied to each element in the input map and the function returns the value associated with the key 'question'. (get) is safer for accessing values in a dictionary because it handles the case where the key might not exist.\",\n",
    "    \"Use StrOutputParser. from langchain_openai import ChatOpenAI; from langchain_core.prompts import ChatPromptTemplate; from langchain_core.output_parsers import StrOutputParser; prompt = ChatPromptTemplate.from_template('Tell me a short joke about {topic}'); model = ChatOpenAI(model='gpt-3.5-turbo') #gpt-4 or other LLMs can be used here; output_parser = StrOutputParser(); chain = prompt | model | output_parser\",\n",
    "    \"We can use RunnableParallel. For example: from langchain_core.prompts import ChatPromptTemplate; from langchain_core.runnables import RunnableParallel; from langchain_openai import ChatOpenAI; model = ChatOpenAI(); joke_chain = ChatPromptTemplate.from_template('tell me a joke about {topic}') | model; poem_chain = (ChatPromptTemplate.from_template('write a 2-line poem about {topic}') | model); map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain); map_chain.invoke({'topic': 'bear'})\",\n",
    "]\n",
    "\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "\n",
    "# Create dataset\n",
    "client = Client()\n",
    "dataset_name = \"RAG_QA_LCEL_Reg_Testing\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about LCEL.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "407cc276-b76b-4e4d-bb3d-339d19ee225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INDEX\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load\n",
    "url = \"https://python.langchain.com/v0.1/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a064272-77fc-4b2f-be6a-7fc6e42b185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAG\n",
    "from langsmith import traceable\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class RagBot_RT:\n",
    "    \"\"\"\n",
    "    A class to interface with retrieval-augmented generation (RAG) models from different providers\n",
    "    such as Mistral or Gemini, utilizing a retriever for document-based context.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        retriever,\n",
    "        provider: str = \"mistral\",\n",
    "        model: str = \"mistral-large-latest\",\n",
    "        use_vectorstore: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the RagBot with a retriever, provider information, model details, and configuration\n",
    "        to use a vector store for document retrieval.\n",
    "\n",
    "        Args:\n",
    "        retriever: The document retriever instance.\n",
    "        provider (str): The provider of the RAG model ('Mistral' or 'Gemini').\n",
    "        model (str): The model identifier used by the provider.\n",
    "        use_vectorstore (bool): Flag to determine whether to use vectorstore for document retrieval.\n",
    "        \"\"\"\n",
    "        self._retriever = retriever\n",
    "        self._provider = provider\n",
    "        self._model = model\n",
    "        self._use_vectorstore = use_vectorstore\n",
    "        if provider == \"mistral\":\n",
    "            self._client = Mistral(api_key=mistral_api_key)\n",
    "        elif provider == \"Gemini\":\n",
    "            self._client = ChatGoogleGenerativeAI(model=self._model,temperature=0.3, max_tokens=500)\n",
    "\n",
    "        \n",
    "    @traceable()\n",
    "    def retrieve_docs_RT(self, question):\n",
    "        \"\"\"\n",
    "        Retrieves documents based on the input question, using either a vectorstore or full context.\n",
    "\n",
    "        Args:\n",
    "        question (str): The question to retrieve documents for.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of documents relevant to the question or the full context (as a string).\n",
    "        \"\"\"\n",
    "        if self._use_vectorstore:\n",
    "            return self._retriever.invoke(question)\n",
    "        else:\n",
    "            return full_doc_text\n",
    "\n",
    "    @traceable()\n",
    "    def get_answer_RT(self, question: str):\n",
    "        \"\"\"\n",
    "        Generates an answer for a given question by using RAG, leveraging both the retriever\n",
    "        and the provider's model capabilities.\n",
    "\n",
    "        Args:\n",
    "        question (str): The user's question to answer.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary containing the 'answer' and 'contexts' (related documents).\n",
    "        \"\"\"\n",
    "        similar = self.retrieve_docs_RT(question)\n",
    "        if self._provider == \"mistral\":\n",
    "            \"MistralAI RAG\"\n",
    "            response = self._client.chat.complete(\n",
    "                model=self._model,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful AI code assistant with expertise in LCEL.\\n\"\n",
    "                        \" Use the following docs to produce a concise code solution to the user question.\\n\"\n",
    "                        \" Use three sentences maximum and keep the answer concise. \\n\"\n",
    "                        f\"## Docs\\n\\n{similar}\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": question},\n",
    "                ],\n",
    "            )\n",
    "            response_str = response.choices[0].message.content\n",
    "\n",
    "        elif self._provider == \"Gemini\":\n",
    "            \"Gemini RAG\"\n",
    "            prompt = PromptTemplate(\n",
    "                template=\"\"\"You are a helpful AI code assistant with expertise in LCEL.\n",
    "                Use the following docs to produce a concise code solution to the user question.\n",
    "                If you don't know the answer, just say that you don't know. \n",
    "                Use three sentences maximum and keep the answer concise.\n",
    "                Question: {question} \n",
    "                Context: {context} \n",
    "                Answer: \"\"\",\n",
    "                input_variables=[\"question\", \"context\"],\n",
    "            )\n",
    "            rag_chain = prompt | self._client | StrOutputParser()\n",
    "            response_str = rag_chain.invoke({\"context\": similar, \"question\": question})\n",
    "\n",
    "        return {\n",
    "            \"answer\": response_str,\n",
    "            \"contexts\": [str(doc) for doc in similar],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cfa8cc13-203a-4019-a9db-071447f19203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rag_answer_mistral_RT(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    rag_bot = RagBot_RT(retriever, provider=\"mistral\", model=\"mistral-large-latest\")\n",
    "    response = rag_bot.get_answer_RT(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "\n",
    "def predict_rag_answer_gemini_pro_RT(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    rag_bot = RagBot_RT(retriever, provider=\"Gemini\", model=\"gemini-1.5-pro\")\n",
    "    response = rag_bot.get_answer_RT(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "\n",
    "def predict_rag_answer_gemini_flash_RT(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    rag_bot = RagBot_RT(retriever, provider=\"Gemini\", model=\"gemini-1.5-flash\")\n",
    "    response = rag_bot.get_answer_RT(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "db287994-8673-4a7f-b024-e80ada9bfe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import re\n",
    "\n",
    "def answer_evaluator_RT(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer generation\n",
    "    \"\"\"\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    reference = example.outputs[\"answer\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "    \n",
    "    # LLM\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0.3, max_tokens=500)\n",
    "\n",
    "    # Prompt\n",
    "    system = \"\"\"Is the Assistant's Answer grounded in and similar to the Ground Truth answer. Note that we do not expect all of the text \n",
    "            in code solution examples to be identical. We expect (1) code imports to be identical if the same import is used. (2) But, it is\n",
    "            ok if there are differences in the implementation itself. The main point is that the same concept is employed. A score of 1 means \n",
    "            that the Assistant answer is not at all conceptically grounded in and similar to the Ground Truth answer. A score of 5 means  that the Assistant \n",
    "            answer contains some information that is conceptically grounded in and similar to the Ground Truth answer. A score of 10 means that the \n",
    "            Assistant answer is fully conceptically grounded in and similar to the Ground Truth answer.\n",
    "            return only a numerical score for answer accuracy, a score from 1 to 10\"\"\"\n",
    "\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Ground Truth answer: \\n\\n {reference} \\n\\n Assistant's Answer: {prediction}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    answer_grader = grade_prompt | llm\n",
    "    score = answer_grader.invoke({\"reference\": reference, \"prediction\": prediction})\n",
    "    final_score = int(score.content)\n",
    "    return {\"key\": \"answer_accuracy\", \"score\": final_score/10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5a8a475d-4df6-4d28-b326-796434e048dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "client = Mistral(api_key=mistral_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07f87155-e860-429b-86d9-b78b33c4affe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-mistral-662acd6c' at:\n",
      "https://smith.langchain.com/o/a87e4dfa-61d0-4714-8c72-64f2925b822e/datasets/04f9d8d4-cf06-44a9-930c-4702a1a0073c/compare?selectedSessions=22405e21-dbd7-43c1-99b4-844d2b6c0af4\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dbc6426541a46ae859d33a2a23805f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"RAG_QA_LCEL_Reg_Testing\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_mistral_RT,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator_RT],\n",
    "    experiment_prefix=\"rag-qa-mistral\",\n",
    "    metadata={\"variant\": \"LCEL context, mistral-large-latest\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a85ad1e-c2d8-4f04-b15f-09bf982c2bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-gemini-pro-48bd5e53' at:\n",
      "https://smith.langchain.com/o/a87e4dfa-61d0-4714-8c72-64f2925b822e/datasets/04f9d8d4-cf06-44a9-930c-4702a1a0073c/compare?selectedSessions=c45353db-00e1-4be3-b241-05017cab3774\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec26db76a7ca400d8fe6b2d8f47a854d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_gemini_pro_RT,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator_RT],\n",
    "    experiment_prefix=\"rag-qa-gemini-pro\",\n",
    "    metadata={\"variant\": \"LCEL context, gemini-pro\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bdb449c0-adb8-497c-a099-59a166a93b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-qa-gemini-flash-140ce620' at:\n",
      "https://smith.langchain.com/o/a87e4dfa-61d0-4714-8c72-64f2925b822e/datasets/04f9d8d4-cf06-44a9-930c-4702a1a0073c/compare?selectedSessions=51ec1aed-1a4f-4c49-a38f-19a353896d68\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8da0aeec16242dfa1ad8065844b19dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_gemini_flash_RT,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator_RT],\n",
    "    experiment_prefix=\"rag-qa-gemini-flash\",\n",
    "    metadata={\"variant\": \"LCEL context, gemini-flash\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3d367276-faa9-46ce-8826-15c3f39e6ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"13.png\" width=\"1000\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"13.png\", width=1000, height=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20ee90e-15c8-4c79-bdb6-d15cd935180c",
   "metadata": {},
   "source": [
    "<B><H1>Pairwise evaluations - Paper summerization for Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8ca22a85-7c23-43f6-9870-49b2cea7b044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from requests~=2.32.0->arxiv) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "236488ea-96bd-4275-a5ee-95047d11ca54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (1.25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a63668a9-6ae6-4899-828f-52fa4cb17dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "# Arxiv IDs\n",
    "# phi3, llama3 context extension, jamba, longRope, can llms reason & plan, action learning, roformer, attn is all you need, segment anything, # swin transformer\n",
    "ids = [\n",
    "    \"2404.14219\",\n",
    "    \"2404.19553\",\n",
    "    \"2403.19887\",\n",
    "    \"2402.13753\",\n",
    "    \"2403.04121\",\n",
    "    \"2402.15809\",\n",
    "    \"2104.09864\",\n",
    "    \"1706.03762\",\n",
    "    \"2304.02643\",\n",
    "    \"2111.09883\",\n",
    "]\n",
    "\n",
    "# Load papers\n",
    "docs = []\n",
    "for paper_id in ids:\n",
    "    doc = ArxivLoader(query=paper_id, load_max_docs=1).load()\n",
    "    docs.extend(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246aceaf-716f-4cb0-90c4-8a10b39568f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# Summarization\n",
    "inputs = [d.page_content for d in docs]\n",
    "\n",
    "# Create dataset\n",
    "client = Client()\n",
    "dataset_name = \"Paper_Tweet_Generator\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Papers to summarize\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"text\": d} for d in inputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf32458-afe7-4f8d-a863-9bbe9d153698",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a9473804-dff7-420c-96a2-318012562d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.3.7-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.39 (from langchain_openai)\n",
      "  Downloading langchain_core-0.3.40-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting openai<2.0.0,>=1.58.1 (from langchain_openai)\n",
      "  Downloading openai-1.65.1-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.39->langchain_openai) (0.3.9)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.39->langchain_openai) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.39->langchain_openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.39->langchain_openai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.39->langchain_openai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.39->langchain_openai) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.39->langchain_openai) (2.10.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.6.2)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.58.1->langchain_openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.58.1->langchain_openai)\n",
      "  Downloading jiter-0.8.2-cp310-cp310-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.39->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain_openai) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain_openai) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.39->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.39->langchain_openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.58.1->langchain_openai) (0.4.6)\n",
      "Downloading langchain_openai-0.3.7-py3-none-any.whl (55 kB)\n",
      "Downloading langchain_core-0.3.40-py3-none-any.whl (414 kB)\n",
      "Downloading openai-1.65.1-py3-none-any.whl (472 kB)\n",
      "Downloading tiktoken-0.9.0-cp310-cp310-win_amd64.whl (894 kB)\n",
      "   ---------------------------------------- 0.0/894.0 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 786.4/894.0 kB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 894.0/894.0 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.8.2-cp310-cp310-win_amd64.whl (204 kB)\n",
      "Installing collected packages: jiter, distro, tiktoken, openai, langchain-core, langchain_openai\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.37\n",
      "    Uninstalling langchain-core-0.3.37:\n",
      "      Successfully uninstalled langchain-core-0.3.37\n",
      "Successfully installed distro-1.9.0 jiter-0.8.2 langchain-core-0.3.40 langchain_openai-0.3.7 openai-1.65.1 tiktoken-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a2bb54b9-d02e-49d1-8d5a-d43a8d8fe5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_anthropic\n",
      "  Downloading langchain_anthropic-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting anthropic<1,>=0.47.0 (from langchain_anthropic)\n",
      "  Downloading anthropic-0.49.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.39 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain_anthropic) (0.3.40)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain_anthropic) (2.10.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from anthropic<1,>=0.47.0->langchain_anthropic) (4.6.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from anthropic<1,>=0.47.0->langchain_anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from anthropic<1,>=0.47.0->langchain_anthropic) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from anthropic<1,>=0.47.0->langchain_anthropic) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from anthropic<1,>=0.47.0->langchain_anthropic) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from anthropic<1,>=0.47.0->langchain_anthropic) (4.12.2)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.39->langchain_anthropic) (0.3.9)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.39->langchain_anthropic) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.39->langchain_anthropic) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.39->langchain_anthropic) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.39->langchain_anthropic) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain_anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain_anthropic) (2.27.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.47.0->langchain_anthropic) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.47.0->langchain_anthropic) (1.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.47.0->langchain_anthropic) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.47.0->langchain_anthropic) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<1,>=0.47.0->langchain_anthropic) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.39->langchain_anthropic) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain_anthropic) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain_anthropic) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain_anthropic) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain_anthropic) (0.23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain_anthropic) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain_anthropic) (2.3.0)\n",
      "Downloading langchain_anthropic-0.3.8-py3-none-any.whl (23 kB)\n",
      "Downloading anthropic-0.49.0-py3-none-any.whl (243 kB)\n",
      "Installing collected packages: anthropic, langchain_anthropic\n",
      "Successfully installed anthropic-0.49.0 langchain_anthropic-0.3.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "90eeb0a8-5a41-4f99-8a5d-c8bb368e2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "### Anthropic\n",
    "\n",
    "# Prompt\n",
    "system_tweet_instructions = (\n",
    "    \"<role> You are an assistant that generates Tweets to distill / summarize\"\n",
    "    \" an academic paper. Ensure the summary: (1) has an engaging title, \"\n",
    "    \" (2) provides a bullet point list of main points from the paper, \"\n",
    "    \" (3) utilizes emojis, (4) includes limitations of the approach, and \"\n",
    "    \" (5) highlights in one sentence the key point or innovation in the paper. </role>\"\n",
    ")\n",
    "human = \"Here is a paper to convert into a Tweet: <paper> {paper} </paper>\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_tweet_instructions), (\"human\", human)]\n",
    ")\n",
    "\n",
    "def predict_tweet_gemini_flash(example: dict):\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0.3, max_tokens=500)\n",
    "    tweet_generator_gemini_flash = prompt | llm | StrOutputParser()\n",
    "    response = tweet_generator_gemini_flash.invoke({\"paper\": example[\"text\"]})\n",
    "    return {\"answer\": response}\n",
    "\n",
    "def predict_tweet_mistralai_mll(example: dict):\n",
    "    chat = ChatMistralAI(temperature=0, model=\"mistral-large-latest\")\n",
    "    tweet_generator_mistralai_mll = prompt | chat | StrOutputParser()\n",
    "    response = tweet_generator_mistralai_mll.invoke({\"paper\": example[\"text\"]})\n",
    "    return {\"answer\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8214e5-5270-4ec5-80bc-fe136efac8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith.schemas import Example, Run\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "# Grade prompt\n",
    "grade_prompt = hub.pull(\"rlm/summary-evaluator\")\n",
    "\n",
    "def text_summary_grader(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for text summarization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get summary\n",
    "    summary = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0.3, max_tokens=500)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"summary\": summary})\n",
    "    final_score = score[0][\"args\"][\"Score\"]\n",
    "\n",
    "    return {\"key\": \"summary_engagement_score\", \"score\": final_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "da4f16a1-5e6d-4a73-954e-aeabf26b093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-mistralai\n",
      "  Downloading langchain_mistralai-0.2.7-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.37 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-mistralai) (0.3.40)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-mistralai) (0.21.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-mistralai) (0.27.0)\n",
      "Requirement already satisfied: httpx-sse<1,>=0.3.1 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-mistralai) (0.4.0)\n",
      "Requirement already satisfied: pydantic<3,>=2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-mistralai) (2.10.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (4.6.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain-mistralai) (0.14.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.37->langchain-mistralai) (0.3.9)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.37->langchain-mistralai) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.37->langchain-mistralai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.37->langchain-mistralai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.37->langchain-mistralai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.37->langchain-mistralai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from pydantic<3,>=2->langchain-mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from pydantic<3,>=2->langchain-mistralai) (2.27.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from tokenizers<1,>=0.15.1->langchain-mistralai) (0.29.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2025.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (4.67.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.37->langchain-mistralai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.37->langchain-mistralai) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.37->langchain-mistralai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.37->langchain-mistralai) (0.23.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from anyio->httpx<1,>=0.25.2->langchain-mistralai) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ragamira.shankar\\appdata\\local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (0.4.6)\n",
      "Downloading langchain_mistralai-0.2.7-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: langchain-mistralai\n",
      "Successfully installed langchain-mistralai-0.2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "63eeef75-920b-44bf-967c-2e8867ea2b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'summary-gemini-1.5-flash-6322e0c4' at:\n",
      "https://smith.langchain.com/o/a87e4dfa-61d0-4714-8c72-64f2925b822e/datasets/e5b3d17b-6239-489c-8e8c-f4f5dcce9981/compare?selectedSessions=fe1bee1b-bd5e-4a9e-8b13-9ad4a42172be\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c31c49d2dbb42c981cdbcf28a1f15e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"Paper_Tweet_Generator\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_tweet_gemini_flash,\n",
    "    data=dataset_name,\n",
    "    evaluators=[text_summary_grader],\n",
    "    experiment_prefix=\"summary-gemini-1.5-flash\",\n",
    "    metadata={\"variant\": \"paper summary tweet, gemini-1.5-flash\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "38346b20-3711-4bf4-a878-20dde5354eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'summary-mistral-mll-0f9f2ece' at:\n",
      "https://smith.langchain.com/o/a87e4dfa-61d0-4714-8c72-64f2925b822e/datasets/e5b3d17b-6239-489c-8e8c-f4f5dcce9981/compare?selectedSessions=6a387013-98a7-402b-8c20-455acbcf6191\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701d252dcfc24361b0a1d05b0aab2305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Error running target function: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1914, in _forward\n",
      "    fn(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 629, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 626, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\Temp\\ipykernel_20952\\833916525.py\", line 28, in predict_tweet_mistralai_mll\n",
      "    response = tweet_generator_mistralai_mll.invoke({\"paper\": example[\"text\"]})\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3024, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 284, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 860, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 690, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 925, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 547, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 466, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 463, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 170, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator text_summary_grader> on run 809fd785-1998-4c19-933c-eccd24a49b75: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1634, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 629, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\langsmith\\run_helpers.py\", line 626, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"C:\\Users\\ragamira.shankar\\AppData\\Local\\Temp\\ipykernel_20952\\1139071433.py\", line 17, in text_summary_grader\n",
      "    summary = run.outputs[\"answer\"]\n",
      "KeyError: 'answer'\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n"
     ]
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_tweet_mistralai_mll,\n",
    "    data=dataset_name,\n",
    "    evaluators=[text_summary_grader],\n",
    "    experiment_prefix=\"summary-mistral-mll\",\n",
    "    max_concurrency=2,\n",
    "    metadata={\"variant\": \"paper summary tweet, mistral-mll\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "502f7a4e-b0d0-447b-b0a8-284a8292e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langsmith.schemas import Example, Run\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "def evaluate_pairwise(runs: list, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for pairwise answers to score based on  engagement\n",
    "    \"\"\"\n",
    "\n",
    "    # Store scores\n",
    "    scores = {}\n",
    "    for i, run in enumerate(runs):\n",
    "        scores[run.id] = i\n",
    "\n",
    "    # Runs is the pair of runs for each example\n",
    "    answer_a = runs[0].outputs[\"answer\"]\n",
    "    answer_b = runs[1].outputs[\"answer\"]\n",
    "\n",
    "    # LLM with function call, use highest capacity model\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0.3)\n",
    "\n",
    "    # Structured prompt\n",
    "    grade_prompt = hub.pull(\"rlm/pairwise-evaluation-tweet-summary\")\n",
    "    answer_grader = grade_prompt | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke(\n",
    "        {\n",
    "            \"question\": system_tweet_instructions,\n",
    "            \"answer_a\": answer_a,\n",
    "            \"answer_b\": answer_b,\n",
    "        }\n",
    "    )\n",
    "    final_score = score[0][\"args\"][\"Preference\"]\n",
    "\n",
    "    # Map from the score to the run assisnment\n",
    "    if final_score == 1:  # Assistant A is preferred\n",
    "        scores[runs[0].id] = 1\n",
    "        scores[runs[1].id] = 0\n",
    "    elif final_score == 2:  # Assistant B is preferred\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 1\n",
    "    else:\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 0\n",
    "\n",
    "    return {\"key\": \"ranked_preference\", \"scores\": scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c1fdbb14-c3b0-418e-80d5-2089c8b8e2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/a87e4dfa-61d0-4714-8c72-64f2925b822e/datasets/e5b3d17b-6239-489c-8e8c-f4f5dcce9981/compare?selectedSessions=6e52f796-19bb-4e10-8319-0afebaeaf5ef%2C4fe8df47-ed19-4741-847f-e202cdd89305&comparativeExperiment=a9b68177-2478-478b-af6d-a60e3ea9018f\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f755436dc44660ab309a1cad2fe5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n",
      "Key 'parameters' is not supported in schema, ignoring\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langsmith.evaluation._runner.ComparativeExperimentResults at 0x2f692a09e10>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate_comparative\n",
    "\n",
    "evaluate_comparative(\n",
    "    # Replace the following array with the names or IDs of your experiments\n",
    "    [\"summary-mistral-mll-5160412f\", \"summary-gemini-1.5-flash-cdceef3f\"],\n",
    "    evaluators=[evaluate_pairwise],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c3025ca5-47ff-45c6-8ee0-5496a27dc500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"14.png\" width=\"1000\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"14.png\", width=1000, height=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17813770-0538-4ba4-9d8a-bd3a53bdda70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d408c44-da8e-432c-b67b-dbf847010fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
